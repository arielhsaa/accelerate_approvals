# Quick Azure Deployment

Deploy the complete Payment Approval Acceleration demo to Azure in minutes.

## üöÄ One-Click Deployment

Click the button below to deploy directly to Azure:

[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Farielhsaa%2Faccelerate_approvals%2Fmain%2Fazure%2Ftemplates%2Fdatabricks-template.json)

This will deploy:
- ‚úÖ Azure Databricks Premium Workspace
- ‚úÖ Azure Storage Account (ADLS Gen2)
- ‚úÖ Azure Key Vault
- ‚úÖ All necessary configurations

**Time**: ~10 minutes  
**Cost**: ~$800-1200/month

---

## üõ†Ô∏è Automated CLI Deployment

### Option 1: Bash Script (Fastest)

```bash
# Clone repository
git clone https://github.com/arielhsaa/accelerate_approvals.git
cd accelerate_approvals

# Login to Azure
az login

# Run deployment
chmod +x azure/deploy-azure.sh
./azure/deploy-azure.sh
```

**What happens:**
1. Creates resource group
2. Deploys Databricks workspace
3. Creates storage account
4. Sets up Key Vault
5. Generates configuration files

**Time**: 10-15 minutes

### Option 2: Terraform

```bash
cd azure/terraform

# Initialize Terraform
terraform init

# Preview changes
terraform plan

# Deploy
terraform apply

# Get outputs
terraform output
```

**Benefits:**
- Infrastructure as Code
- Version controlled
- Repeatable deployments
- Easy to modify

---

## üìã Step-by-Step Instructions

### Prerequisites

‚úÖ Azure subscription with Contributor access  
‚úÖ Azure CLI installed (`az --version`)  
‚úÖ Logged into Azure (`az login`)

### Step 1: Deploy Infrastructure (10 min)

```bash
# Set your preferred configuration
export RESOURCE_GROUP="payment-approvals-rg"
export LOCATION="eastus"
export WORKSPACE_NAME="payment-approvals-ws"

# Run deployment
./azure/deploy-azure.sh
```

### Step 2: Get Databricks Token (2 min)

1. Open the Databricks workspace URL (shown in output)
2. Go to **Settings** ‚Üí **Developer** ‚Üí **Access Tokens**
3. Click **Generate New Token**
4. Copy the token

### Step 3: Update Configuration (1 min)

```bash
# Edit .env file
nano .env

# Add your token:
DATABRICKS_TOKEN=dapi123abc456def789...
```

### Step 4: Deploy Application Code (5 min)

```bash
# Install Databricks CLI
pip install databricks-cli

# Configure CLI
databricks configure --token

# Deploy notebooks and code
./scripts/deploy.sh
```

### Step 5: Initialize Data (5 min)

1. Open Databricks workspace
2. Navigate to: `/Shared/payment_approval_optimization/notebooks/01_data_ingestion/01_bronze_ingestion`
3. Click **Run All**

### Step 6: Start Streaming (2 min)

1. Open: `01_data_ingestion/02_streaming_pipeline`
2. Click **Run All**

### Step 7: Train Models (3 min)

1. Open: `03_smart_checkout/01_ml_model_training`
2. Click **Run All**

---

## ‚úÖ Verification

Check everything is working:

```bash
python scripts/verify_deployment.py
```

Expected output:
```
‚úì Cluster running
‚úì Notebooks uploaded
‚úì Data generated
‚úì Models trained
‚úì Streaming active
```

---

## üéØ What You Get

### Infrastructure
- **Databricks Premium Workspace** (with MLflow, Delta Lake)
- **Storage Account** (ADLS Gen2 with 1TB capacity)
- **Key Vault** (for secrets management)

### Application
- **6 Notebooks** (data ingestion, ML training, analytics)
- **3 ML Models** (92% AUC approval predictor)
- **3 Dashboards** (real-time monitoring)
- **Streamlit App** (interactive demo)
- **AI Agent** (natural language insights)

### Data
- **100K cardholders** with risk profiles
- **5K merchants** across categories
- **500K+ transactions** with realistic patterns

---

## üí∞ Cost Breakdown

| Resource | Configuration | Monthly Cost |
|----------|--------------|--------------|
| Databricks Premium | 8 nodes, 8hrs/day | $800-1200 |
| Storage (ADLS Gen2) | 1TB data | $20-30 |
| Key Vault | Standard | $5 |
| **Total** | | **$825-1235** |

### Cost Optimization Tips

1. **Use auto-termination** (30 min idle)
2. **Enable auto-scaling** (2-8 workers)
3. **Use spot instances** for dev/test
4. **Archive old data** to cool tier
5. **Schedule jobs** for off-peak hours

**Savings**: Up to 40% reduction

---

## üîí Security Features

‚úÖ **Data Encryption**
- At rest (Azure Storage SSE)
- In transit (TLS 1.2)

‚úÖ **Access Control**
- Azure RBAC
- Databricks ACLs
- Key Vault secrets

‚úÖ **Network Security**
- Private endpoints (optional)
- VNet injection (optional)
- NSG rules

‚úÖ **Compliance**
- PCI-DSS ready
- GDPR compliant
- SOC 2 certified

---

## üìä Monitoring

### Azure Monitor Integration

```bash
# Enable monitoring
az monitor diagnostic-settings create \
  --name databricks-diagnostics \
  --resource <workspace-id> \
  --logs '[{"category": "clusters","enabled": true}]'
```

### Cost Alerts

```bash
# Set budget alert
az consumption budget create \
  --budget-name monthly-budget \
  --amount 1000 \
  --time-grain Monthly
```

### Performance Metrics

- Cluster utilization
- Job run times
- Query performance
- Storage usage

---

## üêõ Troubleshooting

### Deployment Fails

```bash
# Check deployment logs
az deployment group show \
  --resource-group payment-approvals-rg \
  --name deployment-name
```

### Cannot Access Workspace

- Verify you're logged in: `az account show`
- Check firewall rules
- Verify subscription permissions

### Storage Mount Fails

```python
# In Databricks, check mounts
display(dbutils.fs.mounts())

# Remount if needed
dbutils.fs.unmount("/mnt/payment_data")
```

---

## üîÑ Updates & Maintenance

### Update Application Code

```bash
git pull origin main
./scripts/deploy.sh
```

### Scale Resources

```bash
# Increase cluster size
databricks clusters edit --cluster-id <id> --num-workers 16
```

### Backup Data

```bash
# Backup Delta tables
databricks jobs create --json-file backup-job.json
```

---

## üóëÔ∏è Cleanup

### Delete All Resources

```bash
# Option 1: Delete resource group (removes everything)
az group delete --name payment-approvals-rg --yes --no-wait

# Option 2: Terraform destroy
cd azure/terraform
terraform destroy
```

**Warning**: This permanently deletes all data!

---

## üìû Support

### Documentation
- [Full Deployment Guide](AZURE_DEPLOYMENT.md)
- [Quick Start](../QUICKSTART.md)
- [Demo Guide](../docs/demo_guide.md)

### Resources
- [Azure Databricks Docs](https://docs.microsoft.com/azure/databricks/)
- [Terraform Registry](https://registry.terraform.io/providers/hashicorp/azurerm/)
- [GitHub Issues](https://github.com/arielhsaa/accelerate_approvals/issues)

### Community
- Stack Overflow: `[azure-databricks]`
- Databricks Community Forum
- Azure Tech Community

---

## üéâ Success Criteria

You're ready to demo when you can:

‚úÖ Access Databricks workspace  
‚úÖ See data in Delta tables  
‚úÖ Run ML models  
‚úÖ View real-time dashboards  
‚úÖ Calculate ROI ($9.6M impact)

**Total Time: 30 minutes from zero to demo!** ‚ö°

---

## üöÄ Next Steps

1. **Customize**: Adjust configurations in `config/app_config.yaml`
2. **Integrate**: Connect to real data sources
3. **Scale**: Increase cluster size for production
4. **Monitor**: Set up alerts and dashboards
5. **Optimize**: Fine-tune ML models
6. **Present**: Use demo guide to showcase solution

**Ready to accelerate your payment approvals!** üí≥‚ú®
